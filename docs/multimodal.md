# RAG chat: Using GPT vision model with RAG approach

TODO: UPDATE THIS!

[ðŸ“º Watch: (RAG Deep Dive series) Multimedia data ingestion](https://www.youtube.com/watch?v=5FfIy7G2WW0)

This repository includes an optional feature that uses the GPT vision model to generate responses based on retrieved content. This feature is useful for answering questions based on the visual content of documents, such as photos and charts.

## How it works

With this feature enabled, the data ingestion process will extract images from your documents
using Document Intelligence, store the images in Azure Blob Storage, vectorize the images using the Azure AI Vision service, and store the image embeddings in the Azure AI Search index.

During the RAG flow, the app will perform a multi-vector query using both text and image embeddings, and then send any images associated with the retrieved document chunks to the chat completion model for answering questions. This feature assumes that your chat completion model supports multimodal inputs, such as `gpt-4o` or `gpt-4o-mini`.

----OLD----
When this feature is enabled, the following changes are made to the application:

* **Search index**: We added a new field to the Azure AI Search index to store the embedding returned by the multimodal Azure AI Vision API (while keeping the existing field that stores the OpenAI text embeddings).
* **Data ingestion**: In addition to our usual PDF ingestion flow, we also convert each PDF document page to an image, store that image with the filename rendered on top, and add the embedding to the index.
* **Question answering**: We search the index using both the text and multimodal embeddings. We send both the text and the image to gpt-4o, and ask it to answer the question based on both kinds of sources.
* **Citations**: The frontend displays both image sources and text sources, to help users understand how the answer was generated.

For more details on how this feature works, read [this blog post](https://techcommunity.microsoft.com/blog/azuredevcommunityblog/integrating-vision-into-rag-applications/4239460) or watch [this video](https://www.youtube.com/live/C3Zq3z4UQm4?si=SSPowBBJoTBKZ9WW&t=89).

## Using the feature

### Prerequisites

* The use of a chat completion model that supports multimodal inputs. The default model for the repository is currently `gpt-4.1-mini`, which does support multimodal inputs. The `gpt-4o-mini` technically supports multimodal inputs, but due to how image tokens are calculated, you need a much higher deployment capacity to use it effectively. Please try `gpt-4.1-mini` first, and experiment with other models later.

### Deployment

1. **Enable multimodal capabilities:**

   First, make sure you do *not* have integrated vectorization enabled, since that is currently incompatible: (TODO!)

   ```shell
   azd env set USE_FEATURE_INT_VECTORIZATION false
   ```

   Then set the azd environment variable to enable the multimodal feature:

   ```shell
   azd env set USE_MULTIMODAL true
   ```

2. **Provision the multimodal resources:**

   Either run `azd up` if you haven't run it before, or run `azd provision` to provision the multimodal resources. This will create a new Azure AI Vision account and update the Azure AI Search index to include the new image embedding field.

3. **Re-index the data:**

   If you have already indexed data, you will need to re-index it to include the new image embeddings.
   We recommend creating a new Azure AI Search index to avoid conflicts with the existing index.

   ```shell
   azd env set AZURE_SEARCH_INDEX multimodal-index
   ```

   Then run the data ingestion process again to re-index the data:

   Linux/Mac:

   ```shell
   ./scripts/prepdocs.sh
   ```

   Windows:

   ```shell
   .\scripts\prepdocs.ps1
   ```

4. **Try out the feature:**
    ![GPT4V configuration screenshot](./images/gpt4v.png)
   * Access the developer options in the web app and select "Use GPT vision model".
   * New sample questions will show up in the UI that are based on the sample financial document.
   * Try out a question and see the answer generated by the GPT vision model.
   * Check the 'Thought process' and 'Supporting content' tabs.

5. **Customize the multimodal approach:**

   You can customize the RAG flow approach with a few additional environment variables.

   The following variables can be set to either true or false,
   to control whether Azure AI Search will use text embeddings, image embeddings, or both:

   ```shell
   azd env set RAG_SEARCH_TEXT_EMBEDDINGS true
   ```

   ```shell
   azd env set RAG_SEARCH_IMAGE_EMBEDDINGS true
   ```

   The following variable can be set to either true or false,
   to control whether the chat completion model will use text inputs, image inputs, or both:

   ```shell
   azd env set RAG_SEND_TEXT_SOURCES true
   ```

   ```shell
   azd env set RAG_SEND_IMAGE_SOURCES true
   ```

   You can also modify those settings in the "Developer Settings" in the chat UI,
   to experiment with different options before committing to them.

## Compatibility

* This feature is **not** fully compatible with the [agentic retrieval](./agentic_retrieval.md) feature.
The agent *will* perform the multimodal vector embedding search, but it will not return images in the response,
so we cannot send the images to the chat completion model.
* This feature *is* compatible with the [reasoning models](./reasoning.md) feature, as long as you use a model that [supports image inputs](https://learn.microsoft.com/azure/ai-services/openai/how-to/reasoning?tabs=python-secure%2Cpy#api--feature-support).
