TODO:

* Test with integrated vectorization
   * Multivector is working
   * Can we get images mapped??
* Update all TODOs in the code/docs
* Fix/add unit tests - check coverage
* In conftest, should I make a new env for vision? Currently I mashed it into the existing env, but it might be cleaner to have a separate one, as now I have to pass llm_inputs explicitly in the tests to turn off image responses.
   * vote: make a new env
* LLMInputType and VectorFields have inconsistently named values
   * # Vector fields:
      # [X] text embedding field (embedding3) use_text_vector=True
      # [X] image embedding field (images/embedding) use_image_vector=True

      # Inputs for LLM:
      # [X] text sources , use_text_sources = True
      # [X] image sources , use_image_sources = True

* Should we make an Azure AI Vision client class? So we dont have to pass two things around, just one?
   * vision_endpoint and vision_token_provider
   * we have one! its in embeddings.py, add compute_image_embeddings method to it and use it instead

To decide:
* For user data lake client, how often should we double check the ACL matches the oid, versus assuming the URLs convey that? (Like when fetching the image?)
   * add a note that we only check owner, not full access control
   * add owner check to each blob-related method (/list, /remove)
   * move fetch image into blobmanager
   * WHY ARE THERE NO OIDS on the uploaded sections????

Later:
Agentic: Incompatible since it doesnt retrieve images. We would need to do a follow-up search query to get each document, like filter: id eq 'x' or id eq 'y' or....


"id":
"feb5e192afb6_aHR0cHM6Ly9zdHh4azRxenEzdGFoaWMyLmJsb2IuY29yZS53aW5kb3dzLm5ldC9jb250ZW50L05vcnRod2luZF9IZWFsdGhfUGx1c19CZW5lZml0c19EZXRhaWxzLnBkZg2_pages_65",
"aHR0cHM6Ly9zdHh4azRxenEzdGFoaWMyLmJsb2IuY29yZS53aW5kb3dzLm5ldC9jb250ZW50L05vcnRod2luZF9IZWFsdGhfUGx1c19CZW5lZml0c19EZXRhaWxzLnBkZg2",
